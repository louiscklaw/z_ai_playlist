{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Selector Group Chat"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "from typing import List, Sequence\n",
                "\n",
                "from autogen_agentchat.agents import AssistantAgent, UserProxyAgent\n",
                "from autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination\n",
                "from autogen_agentchat.messages import BaseAgentEvent, BaseChatMessage\n",
                "from autogen_agentchat.teams import SelectorGroupChat\n",
                "from autogen_agentchat.ui import Console\n",
                "from autogen_ext.models.openai import OpenAIChatCompletionClient"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Note: This example uses mock tools instead of real APIs for demonstration purposes\n",
                "def search_web_tool(query: str) -> str:\n",
                "    if \"2006-2007\" in query:\n",
                "        return \"\"\"Here are the total points scored by Miami Heat players in the 2006-2007 season:\n",
                "        Udonis Haslem: 844 points\n",
                "        Dwayne Wade: 1397 points\n",
                "        James Posey: 550 points\n",
                "        ...\n",
                "        \"\"\"\n",
                "    elif \"2007-2008\" in query:\n",
                "        return \"The number of total rebounds for Dwayne Wade in the Miami Heat season 2007-2008 is 214.\"\n",
                "    elif \"2008-2009\" in query:\n",
                "        return \"The number of total rebounds for Dwayne Wade in the Miami Heat season 2008-2009 is 398.\"\n",
                "    return \"No data found.\"\n",
                "\n",
                "\n",
                "def percentage_change_tool(start: float, end: float) -> float:\n",
                "    return ((end - start) / start) * 100"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "text_mention_termination = TextMentionTermination(\"TERMINATE\")\n",
                "max_messages_termination = MaxMessageTermination(max_messages=25)\n",
                "termination = text_mention_termination | max_messages_termination"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "selector_prompt = \"\"\"\n",
                "Select an agent to perform task.\n",
                "\n",
                "{roles}\n",
                "\n",
                "Current conversation context:\n",
                "{history}\n",
                "\n",
                "Read the above conversation, then select an agent from {participants} to perform the next task.\n",
                "When the task is complete, let the user approve or disapprove the task.\n",
                "\"\"\".strip()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os,sys\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "\n",
                "def configure():\n",
                "    load_dotenv()\n",
                "    return os.getenv(\"OPENROUTER_API_KEY\")\n",
                "\n",
                "model_client = OpenAIChatCompletionClient(\n",
                "    model=\"GLM-4.5-Flash\",\n",
                "    api_key=configure(),\n",
                "    # base_url=\"...\",\n",
                "    base_url=\"https://api.z.ai/api/paas/v4/\",\n",
                "    temperature=0.8,\n",
                "    # max_tokens=20000,\n",
                "    model_info={\n",
                "        \"vision\": False,\n",
                "        \"function_calling\": True,\n",
                "        \"json_output\": True,\n",
                "        \"family\": \"unknown\",\n",
                "        \"structured_output\": True,\n",
                "    },\n",
                ")\n",
                "\n",
                "planning_agent = AssistantAgent(\n",
                "    \"PlanningAgent\",\n",
                "    description=\"An agent for planning tasks, this agent should be the first to engage when given a new task.\",\n",
                "    model_client=model_client,\n",
                "    system_message=\"\"\"\n",
                "    You are a planning agent.\n",
                "    Your job is to break down complex tasks into smaller, manageable subtasks.\n",
                "    Your team members are:\n",
                "        WebSearchAgent: Searches for information\n",
                "        DataAnalystAgent: Performs calculations\n",
                "\n",
                "    You only plan and delegate tasks - you do not execute them yourself.\n",
                "\n",
                "    When assigning tasks, use this format:\n",
                "    1. <agent> : <task>\n",
                "\n",
                "    After all tasks are complete, summarize the findings and end with \"TERMINATE\".\n",
                "    \"\"\",\n",
                ")\n",
                "\n",
                "web_search_agent = AssistantAgent(\n",
                "    \"WebSearchAgent\",\n",
                "    description=\"An agent for searching information on the web.\",\n",
                "    tools=[search_web_tool],\n",
                "    model_client=model_client,\n",
                "    system_message=\"\"\"\n",
                "    You are a web search agent.\n",
                "    Your only tool is search_tool - use it to find information.\n",
                "    You make only one search call at a time.\n",
                "    Once you have the results, you never do calculations based on them.\n",
                "    \"\"\",\n",
                ")\n",
                "\n",
                "data_analyst_agent = AssistantAgent(\n",
                "    \"DataAnalystAgent\",\n",
                "    description=\"An agent for performing calculations.\",\n",
                "    model_client=model_client,\n",
                "    tools=[percentage_change_tool],\n",
                "    system_message=\"\"\"\n",
                "    You are a data analyst.\n",
                "    Given the tasks you have been assigned, you should analyze the data and provide results using the tools provided.\n",
                "    If you have not seen the data, ask for it.\n",
                "    \"\"\",\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "task = \"Who was the Miami Heat player with the highest points in the 2006-2007 season, and what was the percentage change in his total rebounds between the 2007-2008 and 2008-2009 seasons?\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "---------- TextMessage (user) ----------\n",
                        "Who was the Miami Heat player with the highest points in the 2006-2007 season, and what was the percentage change in his total rebounds between the 2007-2008 and 2008-2009 seasons?\n",
                        "---------- ThoughtEvent (PlanningAgent) ----------\n",
                        "I need to break down this complex task into smaller, manageable subtasks. Let me analyze what information I need to find:\n",
                        "\n",
                        "1. Identify the Miami Heat player with the highest points in the 2006-2007 season.\n",
                        "2. Find that player's total rebounds in the 2007-2008 season.\n",
                        "3. Find that player's total rebounds in the 2008-2009 season.\n",
                        "4. Calculate the percentage change in rebounds between those two seasons.\n",
                        "\n",
                        "This task requires finding specific basketball statistics, which means I'll need to use a web search to find this information. Let me break this down into the subtasks:\n",
                        "\n",
                        "1. WebSearchAgent: Search for Miami Heat player with highest points in 2006-2007 season\n",
                        "2. Once we identify that player, WebSearchAgent: Find that player's total rebounds in 2007-2008 season\n",
                        "3. WebSearchAgent: Find that player's total rebounds in 2008-2009 season\n",
                        "4. DataAnalystAgent: Calculate the percentage change in rebounds between 2007-2008 and 2008-2009 seasons\n",
                        "\n",
                        "Let me proceed with assigning these tasks.\n",
                        "---------- TextMessage (PlanningAgent) ----------\n",
                        "\n",
                        "1. WebSearchAgent: Find the Miami Heat player with the highest points in the 2006-2007 season\n",
                        "2. WebSearchAgent: Get the total rebounds for that player in the 2007-2008 season\n",
                        "3. WebSearchAgent: Get the total rebounds for that player in the 2008-2009 season\n",
                        "4. DataAnalystAgent: Calculate the percentage change in rebounds between the 2007-2008 and 2008-2009 seasons\n",
                        "---------- TextMessage (UserProxyAgent) ----------\n",
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Error processing publish message for PlanningAgent_2d3bc413-e73d-4956-aadd-cd662ca5be21/2d3bc413-e73d-4956-aadd-cd662ca5be21\n",
                        "Traceback (most recent call last):\n",
                        "  File \"/home/logic/miniconda3/envs/z_ai_tryout/lib/python3.11/site-packages/autogen_core/_single_threaded_agent_runtime.py\", line 606, in _on_message\n",
                        "    return await agent.on_message(\n",
                        "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"/home/logic/miniconda3/envs/z_ai_tryout/lib/python3.11/site-packages/autogen_core/_base_agent.py\", line 119, in on_message\n",
                        "    return await self.on_message_impl(message, ctx)\n",
                        "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"/home/logic/miniconda3/envs/z_ai_tryout/lib/python3.11/site-packages/autogen_agentchat/teams/_group_chat/_sequential_routed_agent.py\", line 67, in on_message_impl\n",
                        "    return await super().on_message_impl(message, ctx)\n",
                        "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"/home/logic/miniconda3/envs/z_ai_tryout/lib/python3.11/site-packages/autogen_core/_routed_agent.py\", line 485, in on_message_impl\n",
                        "    return await h(self, message, ctx)\n",
                        "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"/home/logic/miniconda3/envs/z_ai_tryout/lib/python3.11/site-packages/autogen_core/_routed_agent.py\", line 268, in wrapper\n",
                        "    return_value = await func(self, message, ctx)  # type: ignore\n",
                        "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"/home/logic/miniconda3/envs/z_ai_tryout/lib/python3.11/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py\", line 133, in handle_request\n",
                        "    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\n",
                        "  File \"/home/logic/miniconda3/envs/z_ai_tryout/lib/python3.11/site-packages/autogen_agentchat/agents/_assistant_agent.py\", line 953, in on_messages_stream\n",
                        "    async for inference_output in self._call_llm(\n",
                        "  File \"/home/logic/miniconda3/envs/z_ai_tryout/lib/python3.11/site-packages/autogen_agentchat/agents/_assistant_agent.py\", line 1107, in _call_llm\n",
                        "    model_result = await model_client.create(\n",
                        "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"/home/logic/miniconda3/envs/z_ai_tryout/lib/python3.11/site-packages/autogen_ext/models/openai/_openai_client.py\", line 691, in create\n",
                        "    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future\n",
                        "                                                                     ^^^^^^^^^^^^\n",
                        "  File \"/home/logic/miniconda3/envs/z_ai_tryout/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 2556, in create\n",
                        "    return await self._post(\n",
                        "           ^^^^^^^^^^^^^^^^^\n",
                        "  File \"/home/logic/miniconda3/envs/z_ai_tryout/lib/python3.11/site-packages/openai/_base_client.py\", line 1794, in post\n",
                        "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
                        "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"/home/logic/miniconda3/envs/z_ai_tryout/lib/python3.11/site-packages/openai/_base_client.py\", line 1594, in request\n",
                        "    raise self._make_status_error_from_response(err.response) from None\n",
                        "openai.BadRequestError: Error code: 400 - {'error': {'code': '1213', 'message': 'The prompt parameter was not received normally.'}}\n",
                        "Error processing publish message for WebSearchAgent_2d3bc413-e73d-4956-aadd-cd662ca5be21/2d3bc413-e73d-4956-aadd-cd662ca5be21\n",
                        "Traceback (most recent call last):\n",
                        "  File \"/home/logic/miniconda3/envs/z_ai_tryout/lib/python3.11/site-packages/autogen_core/_single_threaded_agent_runtime.py\", line 606, in _on_message\n",
                        "    return await agent.on_message(\n",
                        "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"/home/logic/miniconda3/envs/z_ai_tryout/lib/python3.11/site-packages/autogen_core/_base_agent.py\", line 119, in on_message\n",
                        "    return await self.on_message_impl(message, ctx)\n",
                        "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"/home/logic/miniconda3/envs/z_ai_tryout/lib/python3.11/site-packages/autogen_agentchat/teams/_group_chat/_sequential_routed_agent.py\", line 72, in on_message_impl\n",
                        "    return await super().on_message_impl(message, ctx)\n",
                        "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"/home/logic/miniconda3/envs/z_ai_tryout/lib/python3.11/site-packages/autogen_core/_routed_agent.py\", line 486, in on_message_impl\n",
                        "    return await self.on_unhandled_message(message, ctx)  # type: ignore\n",
                        "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"/home/logic/miniconda3/envs/z_ai_tryout/lib/python3.11/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py\", line 195, in on_unhandled_message\n",
                        "    raise ValueError(f\"Unhandled message in agent container: {type(message)}\")\n",
                        "ValueError: Unhandled message in agent container: <class 'autogen_agentchat.teams._group_chat._events.GroupChatError'>\n",
                        "Error processing publish message for DataAnalystAgent_2d3bc413-e73d-4956-aadd-cd662ca5be21/2d3bc413-e73d-4956-aadd-cd662ca5be21\n",
                        "Traceback (most recent call last):\n",
                        "  File \"/home/logic/miniconda3/envs/z_ai_tryout/lib/python3.11/site-packages/autogen_core/_single_threaded_agent_runtime.py\", line 606, in _on_message\n",
                        "    return await agent.on_message(\n",
                        "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"/home/logic/miniconda3/envs/z_ai_tryout/lib/python3.11/site-packages/autogen_core/_base_agent.py\", line 119, in on_message\n",
                        "    return await self.on_message_impl(message, ctx)\n",
                        "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"/home/logic/miniconda3/envs/z_ai_tryout/lib/python3.11/site-packages/autogen_agentchat/teams/_group_chat/_sequential_routed_agent.py\", line 72, in on_message_impl\n",
                        "    return await super().on_message_impl(message, ctx)\n",
                        "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"/home/logic/miniconda3/envs/z_ai_tryout/lib/python3.11/site-packages/autogen_core/_routed_agent.py\", line 486, in on_message_impl\n",
                        "    return await self.on_unhandled_message(message, ctx)  # type: ignore\n",
                        "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"/home/logic/miniconda3/envs/z_ai_tryout/lib/python3.11/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py\", line 195, in on_unhandled_message\n",
                        "    raise ValueError(f\"Unhandled message in agent container: {type(message)}\")\n",
                        "ValueError: Unhandled message in agent container: <class 'autogen_agentchat.teams._group_chat._events.GroupChatError'>\n",
                        "Error processing publish message for UserProxyAgent_2d3bc413-e73d-4956-aadd-cd662ca5be21/2d3bc413-e73d-4956-aadd-cd662ca5be21\n",
                        "Traceback (most recent call last):\n",
                        "  File \"/home/logic/miniconda3/envs/z_ai_tryout/lib/python3.11/site-packages/autogen_core/_single_threaded_agent_runtime.py\", line 606, in _on_message\n",
                        "    return await agent.on_message(\n",
                        "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"/home/logic/miniconda3/envs/z_ai_tryout/lib/python3.11/site-packages/autogen_core/_base_agent.py\", line 119, in on_message\n",
                        "    return await self.on_message_impl(message, ctx)\n",
                        "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"/home/logic/miniconda3/envs/z_ai_tryout/lib/python3.11/site-packages/autogen_agentchat/teams/_group_chat/_sequential_routed_agent.py\", line 72, in on_message_impl\n",
                        "    return await super().on_message_impl(message, ctx)\n",
                        "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"/home/logic/miniconda3/envs/z_ai_tryout/lib/python3.11/site-packages/autogen_core/_routed_agent.py\", line 486, in on_message_impl\n",
                        "    return await self.on_unhandled_message(message, ctx)  # type: ignore\n",
                        "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"/home/logic/miniconda3/envs/z_ai_tryout/lib/python3.11/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py\", line 195, in on_unhandled_message\n",
                        "    raise ValueError(f\"Unhandled message in agent container: {type(message)}\")\n",
                        "ValueError: Unhandled message in agent container: <class 'autogen_agentchat.teams._group_chat._events.GroupChatError'>\n"
                    ]
                },
                {
                    "ename": "RuntimeError",
                    "evalue": "BadRequestError: Error code: 400 - {'error': {'code': '1213', 'message': 'The prompt parameter was not received normally.'}}\nTraceback:\nTraceback (most recent call last):\n\n  File \"/home/logic/miniconda3/envs/z_ai_tryout/lib/python3.11/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py\", line 133, in handle_request\n    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\n\n  File \"/home/logic/miniconda3/envs/z_ai_tryout/lib/python3.11/site-packages/autogen_agentchat/agents/_assistant_agent.py\", line 953, in on_messages_stream\n    async for inference_output in self._call_llm(\n\n  File \"/home/logic/miniconda3/envs/z_ai_tryout/lib/python3.11/site-packages/autogen_agentchat/agents/_assistant_agent.py\", line 1107, in _call_llm\n    model_result = await model_client.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/home/logic/miniconda3/envs/z_ai_tryout/lib/python3.11/site-packages/autogen_ext/models/openai/_openai_client.py\", line 691, in create\n    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future\n                                                                     ^^^^^^^^^^^^\n\n  File \"/home/logic/miniconda3/envs/z_ai_tryout/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 2556, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n\n  File \"/home/logic/miniconda3/envs/z_ai_tryout/lib/python3.11/site-packages/openai/_base_client.py\", line 1794, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/home/logic/miniconda3/envs/z_ai_tryout/lib/python3.11/site-packages/openai/_base_client.py\", line 1594, in request\n    raise self._make_status_error_from_response(err.response) from None\n\nopenai.BadRequestError: Error code: 400 - {'error': {'code': '1213', 'message': 'The prompt parameter was not received normally.'}}\n",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Reset the previous agents and run the chat again with the user proxy agent and selector function.\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m team.reset()\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m Console(team.run_stream(task=task))\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/z_ai_tryout/lib/python3.11/site-packages/autogen_agentchat/ui/_console.py:117\u001b[39m, in \u001b[36mConsole\u001b[39m\u001b[34m(stream, no_inline_images, output_stats, user_input_manager)\u001b[39m\n\u001b[32m    113\u001b[39m last_processed: Optional[T] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    115\u001b[39m streaming_chunks: List[\u001b[38;5;28mstr\u001b[39m] = []\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m stream:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, TaskResult):\n\u001b[32m    119\u001b[39m         duration = time.time() - start_time\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/z_ai_tryout/lib/python3.11/site-packages/autogen_agentchat/teams/_group_chat/_base_group_chat.py:554\u001b[39m, in \u001b[36mBaseGroupChat.run_stream\u001b[39m\u001b[34m(self, task, cancellation_token, output_task_messages)\u001b[39m\n\u001b[32m    550\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, GroupChatTermination):\n\u001b[32m    551\u001b[39m     \u001b[38;5;66;03m# If the message contains an error, we need to raise it here.\u001b[39;00m\n\u001b[32m    552\u001b[39m     \u001b[38;5;66;03m# This will stop the team and propagate the error.\u001b[39;00m\n\u001b[32m    553\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m message.error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;28mstr\u001b[39m(message.error))\n\u001b[32m    555\u001b[39m     stop_reason = message.message.content\n\u001b[32m    556\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
                        "\u001b[31mRuntimeError\u001b[39m: BadRequestError: Error code: 400 - {'error': {'code': '1213', 'message': 'The prompt parameter was not received normally.'}}\nTraceback:\nTraceback (most recent call last):\n\n  File \"/home/logic/miniconda3/envs/z_ai_tryout/lib/python3.11/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py\", line 133, in handle_request\n    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\n\n  File \"/home/logic/miniconda3/envs/z_ai_tryout/lib/python3.11/site-packages/autogen_agentchat/agents/_assistant_agent.py\", line 953, in on_messages_stream\n    async for inference_output in self._call_llm(\n\n  File \"/home/logic/miniconda3/envs/z_ai_tryout/lib/python3.11/site-packages/autogen_agentchat/agents/_assistant_agent.py\", line 1107, in _call_llm\n    model_result = await model_client.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/home/logic/miniconda3/envs/z_ai_tryout/lib/python3.11/site-packages/autogen_ext/models/openai/_openai_client.py\", line 691, in create\n    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future\n                                                                     ^^^^^^^^^^^^\n\n  File \"/home/logic/miniconda3/envs/z_ai_tryout/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 2556, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n\n  File \"/home/logic/miniconda3/envs/z_ai_tryout/lib/python3.11/site-packages/openai/_base_client.py\", line 1794, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/home/logic/miniconda3/envs/z_ai_tryout/lib/python3.11/site-packages/openai/_base_client.py\", line 1594, in request\n    raise self._make_status_error_from_response(err.response) from None\n\nopenai.BadRequestError: Error code: 400 - {'error': {'code': '1213', 'message': 'The prompt parameter was not received normally.'}}\n"
                    ]
                }
            ],
            "source": [
                "user_proxy_agent = UserProxyAgent(\"UserProxyAgent\", description=\"A proxy for the user to approve or disapprove tasks.\")\n",
                "\n",
                "\n",
                "def selector_func_with_user_proxy(messages: Sequence[BaseAgentEvent | BaseChatMessage]) -> str | None:\n",
                "    if messages[-1].source != planning_agent.name and messages[-1].source != user_proxy_agent.name:\n",
                "        # Planning agent should be the first to engage when given a new task, or check progress.\n",
                "        return planning_agent.name\n",
                "    if messages[-1].source == planning_agent.name:\n",
                "        if messages[-2].source == user_proxy_agent.name and \"APPROVE\" in messages[-1].content.upper():  # type: ignore\n",
                "            # User has approved the plan, proceed to the next agent.\n",
                "            return None\n",
                "        # Use the user proxy agent to get the user's approval to proceed.\n",
                "        return user_proxy_agent.name\n",
                "    if messages[-1].source == user_proxy_agent.name:\n",
                "        # If the user does not approve, return to the planning agent.\n",
                "        if \"APPROVE\" not in messages[-1].content.upper():  # type: ignore\n",
                "            return planning_agent.name\n",
                "    return None\n",
                "\n",
                "team = SelectorGroupChat(\n",
                "    [planning_agent, web_search_agent, data_analyst_agent, user_proxy_agent],\n",
                "    model_client=model_client,\n",
                "    termination_condition=termination,\n",
                "    selector_prompt=selector_prompt,\n",
                "    selector_func=selector_func_with_user_proxy,\n",
                "    allow_repeated_speaker=True,\n",
                ")\n",
                "\n",
                "# Reset the previous agents and run the chat again with the user proxy agent and selector function.\n",
                "await team.reset()\n",
                "\n",
                "await Console(team.run_stream(task=task))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Using Reasoning Models\n",
                "\n",
                "So far in the examples, we have used a `gpt-4o` model. Models like `gpt-4o`\n",
                "and `gemini-1.5-flash` are great at following instructions, so you can\n",
                "have relatively detailed instructions in the selector prompt for the team and the \n",
                "system messages for each agent to guide their behavior.\n",
                "\n",
                "However, if you are using a reasoning model like `o3-mini`, you will need to\n",
                "keep the selector prompt and system messages as simple and to the point as possible.\n",
                "This is because the reasoning models are already good at coming up with their own \n",
                "instructions given the context provided to them.\n",
                "\n",
                "This also means that we don't need a planning agent to break down the task\n",
                "anymore, since the {py:class}`~autogen_agentchat.teams.SelectorGroupChat` that\n",
                "uses a reasoning model can do that on its own.\n",
                "\n",
                "In the following example, we will use `o3-mini` as the model for the\n",
                "agents and the team, and we will not use a planning agent.\n",
                "Also, we are keeping the selector prompt and system messages as simple as possible."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "# model_client = OpenAIChatCompletionClient(model=\"o3-mini\")\n",
                "\n",
                "web_search_agent = AssistantAgent(\n",
                "    \"WebSearchAgent\",\n",
                "    description=\"An agent for searching information on the web.\",\n",
                "    tools=[search_web_tool],\n",
                "    model_client=model_client,\n",
                "    system_message=\"\"\"Use web search tool to find information.\"\"\",\n",
                ")\n",
                "\n",
                "data_analyst_agent = AssistantAgent(\n",
                "    \"DataAnalystAgent\",\n",
                "    description=\"An agent for performing calculations.\",\n",
                "    model_client=model_client,\n",
                "    tools=[percentage_change_tool],\n",
                "    system_message=\"\"\"Use tool to perform calculation. If you have not seen the data, ask for it.\"\"\",\n",
                ")\n",
                "\n",
                "user_proxy_agent = UserProxyAgent(\n",
                "    \"UserProxyAgent\",\n",
                "    description=\"A user to approve or disapprove tasks.\",\n",
                ")\n",
                "\n",
                "selector_prompt = \"\"\"Select an agent to perform task.\n",
                "\n",
                "{roles}\n",
                "\n",
                "Current conversation context:\n",
                "{history}\n",
                "\n",
                "Read the above conversation, then select an agent from {participants} to perform the next task.\n",
                "When the task is complete, let the user approve or disapprove the task.\n",
                "\"\"\"\n",
                "\n",
                "team = SelectorGroupChat(\n",
                "    [web_search_agent, data_analyst_agent, user_proxy_agent],\n",
                "    model_client=model_client,\n",
                "    termination_condition=termination,  # Use the same termination condition as before.\n",
                "    selector_prompt=selector_prompt,\n",
                "    allow_repeated_speaker=True,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Model failed to select a speaker after 3 and there was no previous speaker, using the first participant.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "---------- TextMessage (user) ----------\n",
                        "Who was the Miami Heat player with the highest points in the 2006-2007 season, and what was the percentage change in his total rebounds between the 2007-2008 and 2008-2009 seasons?\n",
                        "---------- ThoughtEvent (WebSearchAgent) ----------\n",
                        "\n",
                        "\n",
                        "I'll help you find that information. Let me start by searching for the Miami Heat player with the highest points in the 2006-2007 season.\n",
                        "\n",
                        "---------- ToolCallRequestEvent (WebSearchAgent) ----------\n",
                        "[FunctionCall(id='call_-8454584468449555333', arguments='{\"query\": \"Miami Heat leading scorer 2006-2007 season points\"}', name='search_web_tool')]\n",
                        "---------- ToolCallExecutionEvent (WebSearchAgent) ----------\n",
                        "[FunctionExecutionResult(content='Here are the total points scored by Miami Heat players in the 2006-2007 season:\\n        Udonis Haslem: 844 points\\n        Dwayne Wade: 1397 points\\n        James Posey: 550 points\\n        ...\\n        ', name='search_web_tool', call_id='call_-8454584468449555333', is_error=False)]\n",
                        "---------- ToolCallSummaryMessage (WebSearchAgent) ----------\n",
                        "Here are the total points scored by Miami Heat players in the 2006-2007 season:\n",
                        "        Udonis Haslem: 844 points\n",
                        "        Dwayne Wade: 1397 points\n",
                        "        James Posey: 550 points\n",
                        "        ...\n",
                        "        \n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Model failed to select a speaker after 3, using the previous speaker.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "---------- ThoughtEvent (WebSearchAgent) ----------\n",
                        "\n",
                        "\n",
                        "---------- ToolCallRequestEvent (WebSearchAgent) ----------\n",
                        "[FunctionCall(id='call_-8454581719670248096', arguments='{\"query\": \"Dwyane Wade rebounds 2007-2008 season 2008-2009 season\"}', name='search_web_tool')]\n",
                        "---------- ToolCallExecutionEvent (WebSearchAgent) ----------\n",
                        "[FunctionExecutionResult(content='The number of total rebounds for Dwayne Wade in the Miami Heat season 2007-2008 is 214.', name='search_web_tool', call_id='call_-8454581719670248096', is_error=False)]\n",
                        "---------- ToolCallSummaryMessage (WebSearchAgent) ----------\n",
                        "The number of total rebounds for Dwayne Wade in the Miami Heat season 2007-2008 is 214.\n",
                        "---------- ThoughtEvent (WebSearchAgent) ----------\n",
                        "\n",
                        "\n",
                        "---------- ToolCallRequestEvent (WebSearchAgent) ----------\n",
                        "[FunctionCall(id='call_-8454594913811575874', arguments='{\"query\": \"Dwyane Wade rebounds 2008-2009 season Miami Heat\"}', name='search_web_tool')]\n",
                        "---------- ToolCallExecutionEvent (WebSearchAgent) ----------\n",
                        "[FunctionExecutionResult(content='The number of total rebounds for Dwayne Wade in the Miami Heat season 2008-2009 is 398.', name='search_web_tool', call_id='call_-8454594913811575874', is_error=False)]\n",
                        "---------- ToolCallSummaryMessage (WebSearchAgent) ----------\n",
                        "The number of total rebounds for Dwayne Wade in the Miami Heat season 2008-2009 is 398.\n",
                        "---------- ThoughtEvent (DataAnalystAgent) ----------\n",
                        "\n",
                        "Based on the data provided:\n",
                        "\n",
                        "1. **Highest points scorer in 2006-2007 season**: Dwayne Wade with 1,397 points\n",
                        "\n",
                        "2. **Percentage change in Dwayne Wade's rebounds**: I'll calculate the percentage change from 2007-2008 to 2008-2009 seasons.\n",
                        "\n",
                        "---------- ToolCallRequestEvent (DataAnalystAgent) ----------\n",
                        "[FunctionCall(id='call_-8454567494737597737', arguments='{\"start\": 214, \"end\": 398}', name='percentage_change_tool')]\n",
                        "---------- ToolCallExecutionEvent (DataAnalystAgent) ----------\n",
                        "[FunctionExecutionResult(content='85.98130841121495', name='percentage_change_tool', call_id='call_-8454567494737597737', is_error=False)]\n",
                        "---------- ToolCallSummaryMessage (DataAnalystAgent) ----------\n",
                        "85.98130841121495\n",
                        "---------- TextMessage (UserProxyAgent) ----------\n",
                        "APPROVE\n",
                        "---------- TextMessage (UserProxyAgent) ----------\n",
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Model failed to select a speaker after 3, using the previous speaker.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "---------- TextMessage (UserProxyAgent) ----------\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "await Console(team.run_stream(task=task))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```{tip}\n",
                "For more guidance on how to prompt reasoning models, see the\n",
                "Azure AI Services Blog on [Prompt Engineering for OpenAI's O1 and O3-mini Reasoning Models](https://techcommunity.microsoft.com/blog/azure-ai-services-blog/prompt-engineering-for-openai%E2%80%99s-o1-and-o3-mini-reasoning-models/4374010)\n",
                "```"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "z_ai_tryout",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.13"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
